

Part 0_Learning Agent 
Q-Learning attempts to learn the value of being in a given state, and taking a specific action there.

 OpenAI gym provides an easy way for people to experiment with their learning agents in an array of provided toy games.

 # The Frozen Lake 
 The FrozenLake environment consists of a 4x4 grid of blocks, each one either being the start block, the goal block, a safe frozen block, or a dangerous hole. The objective is to have an agent learn to navigate from the start to the goal without moving onto a hole.

 we have 16 possible states (one for each block), and 4 possible actions (the four directions of movement), giving us a 16x4 table of Q-values

 Q-Learning is a table of values for every state (row) and action (column) possible in the environment. 

 We make updates to our Q-table using something called the Bellman equation
 Eq 1. Q(s,a) = r + γ(max(Q(s’,a’))

 This says that the Q-value for a given state (s) and action (a) should represent the current reward (r) plus the maximum discounted (γ) future reward expected according to our own table for the next state (s’) we would end up in. The discount variable allows us to decide how important the possible future rewards are compared to the present reward.


 In the case of the FrozenLake example, we will be using a one-layer network which takes the state encoded in a one-hot vector (1x16), and produces a vector of 4 Q-values, one for each action. 

 Our loss function will be sum-of-squares loss, where the difference between the current predicted Q-values, and the “target” value is computed and the gradients passed through the network. In this case, our Q-target for the chosen action is the equivalent to the Q-value computed in equation 1 above.
Eq2. Loss = ∑(Q-target - Q)²